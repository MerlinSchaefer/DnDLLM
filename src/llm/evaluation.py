from typing import Callable

import mlflow.pyfunc
import pandas as pd
from llama_index.llms.llama_cpp import LlamaCPP


class EvalLLMWrapper(mlflow.pyfunc.PyFuncModel):
    """
    Wrapper for the custom LLM to integrate with MLflow.
    This wrapper ensures compatibility with MLflow's evaluation framework.
    """

    def __init__(self, llm_model, system_prompt=None):
        """
        Initializes the wrapper with the given model and optional system prompt.

        Args:
            llm_model: The LLM model (e.g., Hugging Face pipeline or custom LLM).
            system_prompt: Optional system prompt to use with the model.
        """
        self.llm_model = llm_model
        self.system_prompt = system_prompt

    def predict(self, context, input_data):
        """
        Invokes the LLM model to generate responses.

        Args:
            context: Unused in this wrapper but required for MLflow compatibility.
            input_data: A list of questions or prompts to query the model.

        Returns:
            Model responses for each input.
        """
        if not isinstance(input_data, list):
            raise ValueError("Input data should be a list of strings (questions/prompts).")

        responses = []
        for prompt in input_data:
            # Prepend system prompt if available
            full_prompt = f"{self.system_prompt}\n{prompt}" if self.system_prompt else prompt
            response = self.llm_model.generate(full_prompt)  # Use appropriate method for your LLM
            responses.append(response)

        return responses


def register_llm_model(model_instance, system_prompt=None, model_path="llm_models/custom_model"):
    """
    Saves the custom LLM wrapper as an MLflow pyfunc model.

    :param model_instance: The LLM model to wrap (e.g., Hugging Face or custom implementation).
    :param system_prompt: Optional system prompt for the model.
    :param model_path: Path to save the wrapped model.
    """
    # Wrap the model
    wrapped_model = EvalLLMWrapper(model_instance, system_prompt=system_prompt)

    # Save the model using MLflow
    mlflow.pyfunc.save_model(path=model_path, python_model=wrapped_model)


# Define a function to generate answers using the custom LLM
def llamacpp_qa(inputs: pd.DataFrame, llm: LlamaCPP) -> list[str]:
    """
    Generates answers using the LlamaCPP LLM.

    Args:
        inputs: A DataFrame with a column "inputs" containing questions/prompts.
    Returns:
        A list of responses generated by the LLM.
    """
    predictions = []
    # TODO: Refactor for agent eval
    system_prompt = "You are a helpful assistant."

    for _, row in inputs.iterrows():
        prompt = f"{system_prompt}\n{row['inputs']}"  # Combine system prompt with question
        response = llm.complete(prompt)  # Call LLM
        predictions.append(response)

    return predictions


def create_llamacpp_eval_function(llm: LlamaCPP, system_prompt: str = None) -> Callable:
    """
    Creates a function to evaluate an LLM using MLflow.

    :param llm: The LLM instance, expected to have a method like `generate(prompt)`.
    :param system_prompt: Optional system-level instruction prepended to each prompt.
    :return: A callable function compatible with MLflow's evaluate framework.
    """

    # Define a function to generate answers using the custom LLM
    def llamacpp_qa(inputs: pd.DataFrame) -> list[str]:
        """
        Generates answers using the LlamaCPP LLM.

        Args:
            inputs: A DataFrame with a column "inputs" containing questions/prompts.
        Returns:
            A list of responses generated by the LLM.
        """
        predictions = []
        # TODO: Refactor for agent eval
        system_prompt = "You are a helpful assistant."

        for _, row in inputs.iterrows():
            prompt = f"{system_prompt}\n{row['inputs']}"  # Combine system prompt with question
            response = llm.complete(prompt)  # Call LLM
            predictions.append(response)

        return predictions

    return llamacpp_qa


# Example usage
# from src.llm.deployments import AvailableChatModels, get_chat_model
#
# custom_llm = get_chat_model(
#     model_name=AvailableChatModels.LLAMA_32_3B_Q8,
#     max_new_tokens=8000,
# )
# save_custom_model(custom_llm, system_prompt="You are a helpful assistant.")


# Example dataset for evaluation
# data = {
#     "question": [
#         "What is the capital of France?",
#         "Who wrote 'Pride and Prejudice'?",
#         "What is the boiling point of water at sea level?",
#     ],
#     "answer": ["Paris", "Jane Austen", "100Â°C"],
# }
# evaluation_data = pd.DataFrame(data)
# from mlflow.metrics import exact_match, rougeL

# # Metrics for evaluation
# metrics = {"exact_match": exact_match, "rougeL": rougeL}
