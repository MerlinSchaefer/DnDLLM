{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_url\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_path\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/pydantic/_internal/_fields.py:132: UserWarning: Field \"model_kwargs\" in LlamaCPP has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from deployments import get_chat_model, AvailableChatModels\n",
    "from evaluation import create_llamacpp_eval_function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = pd.DataFrame(\n",
    "    {\n",
    "        \"inputs\": [\n",
    "            \"What is MLflow?\",\n",
    "            \"What is Spark?\",\n",
    "        ],\n",
    "        \"ground_truth\": [\n",
    "            \"MLflow is an open-source platform for managing the end-to-end machine learning (ML) \"\n",
    "            \"lifecycle. It was developed by Databricks, a company that specializes in big data and \"\n",
    "            \"machine learning solutions. MLflow is designed to address the challenges that data \"\n",
    "            \"scientists and machine learning engineers face when developing, training, and deploying \"\n",
    "            \"machine learning models.\",\n",
    "            \"Apache Spark is an open-source, distributed computing system designed for big data \"\n",
    "            \"processing and analytics. It was developed in response to limitations of the Hadoop \"\n",
    "            \"MapReduce computing model, offering improvements in speed and ease of use. Spark \"\n",
    "            \"provides libraries for various tasks such as data ingestion, processing, and analysis \"\n",
    "            \"through its components like Spark SQL for structured data, Spark Streaming for \"\n",
    "            \"real-time data processing, and MLlib for machine learning tasks\",\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/ms101/Desktop/dev/DnDLLM/src/llm/.model_cache/llama-3.2-3b-instruct-q8_0.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 30 key-value pairs and 255 tensors from /mnt/c/Users/ms101/Desktop/dev/DnDLLM/src/llm/.model_cache/llama-3.2-3b-instruct-q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 3B\n",
      "llama_model_loader: - kv   6:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv   7:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv   8:                          llama.block_count u32              = 28\n",
      "llama_model_loader: - kv   9:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  10:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  11:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  12:                 llama.attention.head_count u32              = 24\n",
      "llama_model_loader: - kv  13:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  15:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                 llama.attention.key_length u32              = 128\n",
      "llama_model_loader: - kv  17:               llama.attention.value_length u32              = 128\n",
      "llama_model_loader: - kv  18:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   58 tensors\n",
      "llama_model_loader: - type q8_0:  197 tensors\n",
      "llm_load_vocab: special tokens cache size = 256\n",
      "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 28\n",
      "llm_load_print_meta: n_head           = 24\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 3\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 3.21 B\n",
      "llm_load_print_meta: model size       = 3.18 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4070 Laptop GPU, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.24 MiB\n",
      "llm_load_tensors: offloading 28 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 29/29 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   399.23 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  3255.91 MiB\n",
      ".................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3904\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   427.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  427.00 MiB, K (f16):  213.50 MiB, V (f16):  213.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   256.50 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    13.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 902\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 1 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '7', 'llama.attention.value_length': '128', 'llama.attention.key_length': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'general.basename': 'Llama-3.2', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '24', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Llama 3.2 3B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '3B', 'llama.embedding_length': '3072', 'llama.feed_forward_length': '8192', 'llama.block_count': '28', 'llama.attention.head_count_kv': '8'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = get_chat_model(model_name=AvailableChatModels.LLAMA_32_3B_Q8, max_new_tokens=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "llamacpp_qa = create_llamacpp_eval_function(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/11/16 14:57:25 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n",
      "\n",
      "llama_print_timings:        load time =     872.56 ms\n",
      "llama_print_timings:      sample time =      16.88 ms /   374 runs   (    0.05 ms per token, 22151.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =     872.50 ms /    12 tokens (   72.71 ms per token,    13.75 tokens per second)\n",
      "llama_print_timings:        eval time =    6565.90 ms /   373 runs   (   17.60 ms per token,    56.81 tokens per second)\n",
      "llama_print_timings:       total time =    7742.11 ms /   385 tokens\n",
      "Llama.generate: 9 prefix-match hit, remaining 2 prompt tokens to eval\n",
      "\n",
      "llama_print_timings:        load time =     872.56 ms\n",
      "llama_print_timings:      sample time =       9.48 ms /   211 runs   (    0.04 ms per token, 22257.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =      19.62 ms /     2 tokens (    9.81 ms per token,   101.95 tokens per second)\n",
      "llama_print_timings:        eval time =    3553.33 ms /   210 runs   (   16.92 ms per token,    59.10 tokens per second)\n",
      "llama_print_timings:       total time =    3691.27 ms /   212 tokens\n",
      "2024/11/16 14:57:37 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2024/11/16 14:57:39 WARNING mlflow.metrics.metric_definitions: Cannot calculate toxicity for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/11/16 14:57:39 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'toxicity' because it returned None.\n",
      "2024/11/16 14:57:39 WARNING mlflow.metrics.metric_definitions: Cannot calculate flesch_kincaid for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/11/16 14:57:39 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'flesch_kincaid_grade_level' because it returned None.\n",
      "2024/11/16 14:57:39 WARNING mlflow.metrics.metric_definitions: Cannot calculate ari for non-string inputs. Non-string found for the column specified by the `predictions` parameter or the model output column on row 0. Skipping metric logging.\n",
      "2024/11/16 14:57:39 WARNING mlflow.models.evaluation.default_evaluator: Did not log builtin metric 'ari_grade_level' because it returned None.\n"
     ]
    },
    {
     "ename": "MlflowException",
     "evalue": "Metric 'exact_match': Error:\nValueError(\"Classification metrics can't handle a mix of binary and unknown targets\")\nTraceback (most recent call last):\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 1710, in _test_first_row\n    metric_value = _evaluate_metric(metric_tuple, eval_fn_args)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 670, in _evaluate_metric\n    metric = metric_tuple.function(*eval_fn_args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/metrics/metric_definitions.py\", line 162, in _accuracy_eval_fn\n    acc = accuracy_score(y_true=targets, y_pred=predictions, sample_weight=sample_weight)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py\", line 231, in accuracy_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py\", line 112, in _check_targets\n    raise ValueError(\nValueError: Classification metrics can't handle a mix of binary and unknown targets\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the custom LLM using mlflow.evaluate\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m----> 3\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllamacpp_qa\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mground_truth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Column in eval_data containing ground truth answers\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion-answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Specify task type\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Evaluation result for each data record is available in `results.tables`.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     eval_table \u001b[38;5;241m=\u001b[39m results\u001b[38;5;241m.\u001b[39mtables[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_results_table\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/base.py:1838\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data, model_type, targets, predictions, dataset_path, feature_names, evaluators, evaluator_config, custom_metrics, extra_metrics, custom_artifacts, validation_thresholds, baseline_model, env_manager, model_config, baseline_config, inference_params)\u001b[0m\n\u001b[1;32m   1835\u001b[0m predictions_expected_in_model_output \u001b[38;5;241m=\u001b[39m predictions \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1837\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1838\u001b[0m     evaluate_result \u001b[38;5;241m=\u001b[39m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1839\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1840\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1841\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1842\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1843\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1844\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevaluator_name_to_conf_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1845\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1846\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1849\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions_expected_in_model_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1850\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1851\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, _ServedPyFuncModel):\n",
      "File \u001b[0;32m/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/base.py:1053\u001b[0m, in \u001b[0;36m_evaluate\u001b[0;34m(model, model_type, dataset, run_id, evaluator_name_list, evaluator_name_to_conf_map, custom_metrics, extra_metrics, custom_artifacts, baseline_model, predictions)\u001b[0m\n\u001b[1;32m   1051\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m evaluator\u001b[38;5;241m.\u001b[39mcan_evaluate(model_type\u001b[38;5;241m=\u001b[39mmodel_type, evaluator_config\u001b[38;5;241m=\u001b[39mconfig):\n\u001b[1;32m   1052\u001b[0m         _logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating the model with the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mevaluator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m evaluator.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1053\u001b[0m         eval_result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1055\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1058\u001b[0m \u001b[43m            \u001b[49m\u001b[43mevaluator_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_metrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_metrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcustom_artifacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_artifacts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbaseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbaseline_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m         eval_results\u001b[38;5;241m.\u001b[39mappend(eval_result)\n\u001b[1;32m   1067\u001b[0m _last_failed_evaluator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py:2112\u001b[0m, in \u001b[0;36mDefaultEvaluator.evaluate\u001b[0;34m(self, model_type, dataset, run_id, evaluator_config, model, custom_metrics, extra_metrics, custom_artifacts, baseline_model, predictions, **kwargs)\u001b[0m\n\u001b[1;32m   2110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m baseline_model:\n\u001b[1;32m   2111\u001b[0m         _logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating candidate model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 2112\u001b[0m     evaluation_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_baseline_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m baseline_model:\n\u001b[1;32m   2115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m evaluation_result\n",
      "File \u001b[0;32m/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py:1983\u001b[0m, in \u001b[0;36mDefaultEvaluator._evaluate\u001b[0;34m(self, model, is_baseline_model, **kwargs)\u001b[0m\n\u001b[1;32m   1980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mhas_targets:\n\u001b[1;32m   1981\u001b[0m     eval_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my\n\u001b[0;32m-> 1983\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_baseline_model:\n\u001b[1;32m   1985\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_custom_artifacts(eval_df)\n",
      "File \u001b[0;32m/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py:1743\u001b[0m, in \u001b[0;36mDefaultEvaluator._evaluate_metrics\u001b[0;34m(self, eval_df)\u001b[0m\n\u001b[1;32m   1741\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate_metrics\u001b[39m(\u001b[38;5;28mself\u001b[39m, eval_df):\n\u001b[1;32m   1742\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_order_extra_metrics(eval_df)\n\u001b[0;32m-> 1743\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_first_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m     \u001b[38;5;66;03m# calculate metrics for the full eval_df\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m     input_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX\u001b[38;5;241m.\u001b[39mcopy_to_avoid_mutation()\n",
      "File \u001b[0;32m/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py:1730\u001b[0m, in \u001b[0;36mDefaultEvaluator._test_first_row\u001b[0;34m(self, eval_df)\u001b[0m\n\u001b[1;32m   1725\u001b[0m             exceptions\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m   1726\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_tuple\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: Error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstacktrace_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1727\u001b[0m             )\n\u001b[1;32m   1729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(exceptions) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1730\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(exceptions))\n",
      "\u001b[0;31mMlflowException\u001b[0m: Metric 'exact_match': Error:\nValueError(\"Classification metrics can't handle a mix of binary and unknown targets\")\nTraceback (most recent call last):\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 1710, in _test_first_row\n    metric_value = _evaluate_metric(metric_tuple, eval_fn_args)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/models/evaluation/default_evaluator.py\", line 670, in _evaluate_metric\n    metric = metric_tuple.function(*eval_fn_args)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/mlflow/metrics/metric_definitions.py\", line 162, in _accuracy_eval_fn\n    acc = accuracy_score(y_true=targets, y_pred=predictions, sample_weight=sample_weight)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 213, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py\", line 231, in accuracy_score\n    y_type, y_true, y_pred = _check_targets(y_true, y_pred)\n                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/mnt/c/Users/ms101/Desktop/dev/DnDLLM/env/lib/python3.12/site-packages/sklearn/metrics/_classification.py\", line 112, in _check_targets\n    raise ValueError(\nValueError: Classification metrics can't handle a mix of binary and unknown targets\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the custom LLM using mlflow.evaluate\n",
    "with mlflow.start_run():\n",
    "    results = mlflow.evaluate(\n",
    "        model=llamacpp_qa,\n",
    "        data=eval_data,\n",
    "        targets=\"ground_truth\",  # Column in eval_data containing ground truth answers\n",
    "        model_type=\"question-answering\",  # Specify task type\n",
    "    )\n",
    "\n",
    "\n",
    "    # Evaluation result for each data record is available in `results.tables`.\n",
    "    eval_table = results.tables[\"eval_results_table\"]\n",
    "    print(f\"See evaluation table below: \\n{eval_table}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
